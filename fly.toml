# fly.toml app configuration file generated for llm-describer on 2024-03-20T14:37:02-05:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'llm-describer'
primary_region = 'dfw'

[build]

[env]
OLLAMA_API = 'http://llm-describer-llama.flycast'

[[mounts]]
source = 'data'
destination = '/data'
initial_size = '10gb'

[http_service]
internal_port = 8090
force_https = false
auto_stop_machines = true
auto_start_machines = true
min_machines_running = 0
processes = ['app']

[[vm]]
memory = '1gb'
cpu_kind = 'shared'
cpus = 1
