# fly.toml app configuration file generated for llm-describer-llama on 2024-03-20T13:48:51-05:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'llm-describer-llama'
primary_region = 'ord'

[build]
image = 'ollama/ollama'

[[mounts]]
source = 'models'
destination = '/root/.ollama'
initial_size = '100gb'

[http_service]
internal_port = 11434
auto_stop_machines = true
auto_start_machines = true
min_machines_running = 1
processes = ['app']

[[vm]]
size = 'a100-40gb'
